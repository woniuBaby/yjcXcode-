```
H.264和H.265是视频编码标准的名称。

H.264，也称为AVC（Advanced Video Coding），是一种广泛使用的视频编码标准。它被设计用于通过互联网和无线网络传输视频，并且在高清电视和蓝光光盘等领域也得到了广泛应用。H.264采用先进的压缩技术，可以在保持较高视频质量的同时，减少视频文件的大小，节省存储和传输带宽。

H.265，也称为HEVC（High Efficiency Video Coding），是H.264的后继者，旨在进一步提高视频压缩效率。相较于H.264，H.265可以将视频文件的大小再压缩一半左右，同时仍然保持较高的视频质量。H.265的主要优势在于它可以以更少的数据量传输更高分辨率的视频，因此适合用于4K和8K等高分辨率视频的编码和传输。
```



```
以下是iOS设备上的H.264和H.265硬编码过程：

输入处理：在iOS设备上，硬件编码器首先获取要编码的视频数据。这可能是来自摄像头的实时流或者预先录制的视频文件。

图像处理：编码器会对视频图像进行处理，以确保它们符合编码器的规范。这可能包括色彩空间转换、缩放、旋转和其他过滤器。

编码器配置：编码器会根据要生成的视频流的特定要求进行配置。这可能涉及选择编码参数，如比特率、帧率、分辨率和其他参数。

码流生成：硬编码器将视频图像转换为压缩的视频码流，并将其传递给后续处理步骤。这个过程涉及使用特定的算法和压缩技术来减少视频数据的大小，同时保持视频质量。

输出处理：最后，编码器会将压缩的视频数据传递给设备的存储器或传输协议。这个过程涉及将数据封装为适当的容器格式（如MP4或MOV），并应用必要的标记和元数据。

需要注意的是，硬编码是由iOS设备上的专用硬件完成的，这些硬件是为处理视频数据而设计的。这些硬件通常比通用CPU更高效，能够在更短的时间内处理更多的数据。这意味着在iOS设备上，使用硬编码器处理视频可以提高性能和延长电池寿命
```



```
iOS设备上的H.264和H.265硬解码过程如下：

1. 视频编解码器将视频数据压缩为H.264或H.265格式。
2. iOS设备上的硬件解码器（VideoToolbox框架）读取视频数据，并将其解码为原始像素数据。
3. 解码器将像素数据传递给OpenGL ES或Metal等图形库进行后续的图像处理和显示。

在解码过程中，硬件解码器使用专用硬件来加速解码操作，从而提高解码效率和性能。这与软件解码器不同，软件解码器在CPU上运行，通常需要更多的时间和资源才能完成解码操作。

需要注意的是，iOS设备上的硬件解码器只能支持特定的视频分辨率、帧率和比特率等参数。如果视频的参数不符合硬件解码器的要求，iOS设备将无法使用硬件解码器进行解码，而会使用软件解码器。
```



---

### 一



iOS 要使用VideoToolbox对相机采集流buffer进行视频硬编码,区分H264,H265,写一个方法,返回buffer值,使用OC语言,代码注释添加好

```objc
// Video encoder properties
NSString *const AVVideoCodecKey = @"AVVideoCodecKey";
NSString *const AVVideoWidthKey = @"AVVideoWidthKey";
NSString *const AVVideoHeightKey = @"AVVideoHeightKey";
NSString *const AVVideoCompressionPropertiesKey = @"AVVideoCompressionPropertiesKey";
NSString *const AVVideoAverageBitRateKey = @"AVVideoAverageBitRateKey";
NSString *const AVVideoMaxKeyFrameIntervalKey = @"AVVideoMaxKeyFrameIntervalKey";
NSString *const AVVideoProfileLevelKey = @"AVVideoProfileLevelKey";

// H.264 encoder properties
NSString *const AVVideoProfileLevelH264MainAutoLevel = @"H264_Main_AutoLevel";

// H.265 encoder properties
NSString *const AVVideoProfileLevelH265MainAutoLevel = @"HEVC_Main_AutoLevel";

- (CMSampleBufferRef)encodeVideoBuffer:(CMSampleBufferRef)sampleBuffer codecType:(NSString *)codecType {
    // Create a Video Toolbox session
    VTCompressionSessionRef compressionSession;
    OSStatus status = VTCompressionSessionCreate(NULL, self.videoWidth, self.videoHeight, (__bridge CFStringRef)codecType, NULL, NULL, NULL, videoCompressionOutputCallback, (__bridge void *)self, &compressionSession);
    if (status != noErr) {
        NSLog(@"Failed to create a Video Toolbox session: %d", (int)status);
        return nil;
    }
    
    // Set some compression properties
    NSDictionary *encoderProperties = @{
        AVVideoWidthKey: @(self.videoWidth),
        AVVideoHeightKey: @(self.videoHeight),
        AVVideoCodecKey: codecType,
        AVVideoCompressionPropertiesKey: @{
            AVVideoAverageBitRateKey: @(self.videoBitrate),
            AVVideoMaxKeyFrameIntervalKey: @(self.videoMaxKeyFrameInterval),
            AVVideoProfileLevelKey: [self profileLevelForCodecType:codecType]
        }
    };
    
    VTSessionSetProperties(compressionSession, (__bridge CFDictionaryRef)encoderProperties);
    
    // Get the input pixel buffer
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    
    // Set the presentation timestamp
    CMTime presentationTimestamp = CMTimeMake(CMTimeGetSeconds(CMSampleBufferGetPresentationTimeStamp(sampleBuffer)), 1000);
    
    // Prepare the flags
    CMTimeFlags flags = kCMTimeFlags_Valid;
    
    // Prepare the sample timing info
    CMSampleTimingInfo timingInfo = {
        .duration = CMSampleBufferGetDuration(sampleBuffer),
        .presentationTimeStamp = presentationTimestamp,
        .decodeTimeStamp = kCMTimeInvalid
    };
    
    // Compress the pixel buffer
    CMSampleBufferRef outputBuffer;
    status = VTCompressionSessionEncodeFrame(compressionSession, imageBuffer, presentationTimestamp, kCMTimeInvalid, (__bridge CFDictionaryRef)NULL, NULL, &flags, &outputBuffer);
    if (status != noErr) {
        NSLog(@"Failed to encode a frame: %d", (int)status);
        return nil;
    }
    
    // Cleanup
    VTCompressionSessionInvalidate(compressionSession);
    CFRelease(compressionSession);
    
    return outputBuffer;
}

- (NSString *)profileLevelForCodecType:(NSString *)codecType {
    if ([codecType isEqualToString:(NSString *)kCMVideoCodecType_H264]) {
        return AVVideoProfileLevelH264MainAutoLevel;
    } else if ([codecType isEqualToString:(NSString *)kCMVideoCodecType_HEVC]) {
				return AVVideoProfileLevelH265MainAutoLevel;
		} else {
				return nil;
		}
}

// Callback function for video compression output
void videoCompressionOutputCallback(void *outputCallbackRefCon, void *sourceFrameRefCon, OSStatus status, VTEncodeInfoFlags infoFlags, CMSampleBufferRef sampleBuffer) {
// Get a reference to the VideoEncoder instance
VideoEncoder *videoEncoder = (__bridge VideoEncoder *)outputCallbackRefCon;
  
  // Forward the compressed sample buffer to the delegate
if ([videoEncoder.delegate respondsToSelector:@selector(videoEncoder:didOutputSampleBuffer:)]) {
    [videoEncoder.delegate videoEncoder:videoEncoder didOutputSampleBuffer:sampleBuffer];
}

// Release the sample buffer
CFRelease(sampleBuffer);


该方法接受一个CMSampleBuffer对象和一个编码类型（H.264或H.265）作为参数，并返回一个经过硬编码的CMSampleBuffer对象。方法首先创建一个Video Toolbox会话，然后设置编码器属性和一些其他属性。接下来，方法获取输入像素缓冲区，设置呈现时间戳，准备标志和样本定时信息。最后，方法使用VTCompressionSessionEncodeFrame函数对像素缓冲区进行压缩，并返回压缩后的CMSampleBuffer对象。

此外，方法还定义了一个profileLevelForCodecType函数，该函数接受一个编码类型（H.264或H.265）作为参数，并返回一个适合该编码器类型的视频编码器配置文件和级别。

最后，该方法还定义了一个videoCompressionOutputCallback函数，该函数将压缩的样本缓冲区转发给委托对象，并释放样本缓冲区。

```



---

###  二



要验证硬编的buffer是H265格式，可以使用视频播放器播放压缩后的视频，并检查视频文件的编码格式。可以使用以下方法将硬编后的buffer保存在本地，并播放出来：





```objc
- (void)saveAndPlayEncodedBuffer:(CMSampleBufferRef)sampleBuffer withType:(CMVideoCodecType)codecType {
    // Create a file URL for saving the encoded video
    NSString *documentsPath = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES)[0];
    NSString *fileName = [NSString stringWithFormat:@"%@_%@", codecType == kCMVideoCodecType_H264 ? @"encoded_h264" : @"encoded_h265", [NSDate date]];
    NSString *filePath = [documentsPath stringByAppendingPathComponent:fileName];
    NSURL *fileURL = [NSURL fileURLWithPath:filePath];
    
    // Create a file writer for saving the encoded video to disk
    NSError *error;
    AVAssetWriter *assetWriter = [[AVAssetWriter alloc] initWithURL:fileURL fileType:AVFileTypeMPEG4 error:&error];
    NSParameterAssert(assetWriter);
    if (error) {
        NSLog(@"Error creating asset writer: %@", error.localizedDescription);
        return;
    }
    
    // Get the format description of the sample buffer
    CMFormatDescriptionRef formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer);
    
    // Create a video track with the format description
    NSDictionary *videoSettings = @{
        AVVideoCodecKey: codecType == kCMVideoCodecType_H264 ? AVVideoCodecTypeH264 : AVVideoCodecTypeHEVC,
        AVVideoWidthKey: @1920,
        AVVideoHeightKey: @1080
    };
    AVAssetWriterInput *assetWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:videoSettings sourceFormatHint:formatDescription];
    NSParameterAssert(assetWriterInput);
    NSParameterAssert([assetWriter canAddInput:assetWriterInput]);
    [assetWriter addInput:assetWriterInput];
    
    // Configure the video input
    [assetWriterInput setExpectsMediaDataInRealTime:NO];
    [assetWriterInput setTransform:CGAffineTransformMakeRotation(M_PI_2)];
    
    // Create a pixel buffer adaptor for appending pixel buffers to the asset writer input
    NSDictionary *pixelBufferAttributes = @{
        (id)kCVPixelBufferPixelFormatTypeKey: @(codecType == kCMVideoCodecType_H264 ? kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange : kCVPixelFormatType_420HEVC),
        (id)kCVPixelBufferWidthKey: @1920,
        (id)kCVPixelBufferHeightKey: @1088,
        (id)kCVPixelBufferMetalCompatibilityKey: @YES
    };
    AVAssetWriterInputPixelBufferAdaptor *pixelBufferAdaptor = [AVAssetWriterInputPixelBufferAdaptor assetWriterInputPixelBufferAdaptorWithAssetWriterInput:assetWriterInput sourcePixelBufferAttributes:pixelBufferAttributes];
    NSParameterAssert(pixelBufferAdaptor);
    
    // Start the asset writer
    [assetWriter startWriting];
    [assetWriter startSessionAtSourceTime:CMSampleBufferGetPresentationTimeStamp(sampleBuffer)];
    
    // Append the sample buffer to the asset writer input
    dispatch_queue_t mediaQueue = dispatch_queue_create("mediaQueue", NULL);
    [assetWriterInput requestMediaDataWhenReadyOnQueue:mediaQueue usingBlock:^{
        while ([assetWriterInput isReadyForMoreMediaData]) {
            CMSampleBufferRef nextSampleBuffer = NULL;
            if ([pixelBufferAdaptor assetWriterInputPixelBufferAdaptor:pixelBufferAdaptor appendPixelBuffer:CMS            sampleBuffer toPixelBufferPool:NULL withInputTime:NULL outputTime:NULL]) {
                nextSampleBuffer = sampleBuffer;
            } else {
                NSLog(@"Error appending pixel buffer to asset writer input");
            }
            
            if (nextSampleBuffer) {
                [assetWriterInput appendSampleBuffer:nextSampleBuffer];
            } else {
                [assetWriterInput markAsFinished];
                [assetWriter finishWritingWithCompletionHandler:^{
                    dispatch_async(dispatch_get_main_queue(), ^{
                        // Play the saved video file
                        AVPlayerViewController *playerViewController = [[AVPlayerViewController alloc] init];
                        playerViewController.player = [AVPlayer playerWithURL:fileURL];
                        [self presentViewController:playerViewController animated:YES completion:^{
                            [playerViewController.player play];
                        }];
                    });
                }];
            }
        }
    }];
}

该方法将硬编后的样本缓冲区（sample buffer）写入磁盘中的文件，并创建一个 AVPlayerViewController 实例，用于播放该文件。将 AVPlayerViewController 实例显示为模态视图控制器，然后播放文件。注意，在播放视频之前，需要等待写入完成。

```





---

### 三



iOS 要使用VideoToolbox对相机采集流CMSampleBufferRef进行视频硬编码,区分H264,H265, 写一个方法,返回CMSampleBufferRef值,使用OC语言,代码注释添加好

````objective-c
#import <VideoToolbox/VideoToolbox.h>

// 硬编码会话
VTCompressionSessionRef compressionSession = NULL;

// 视频硬编码方法
- (CMSampleBufferRef)encodeSampleBuffer:(CMSampleBufferRef)sampleBuffer codecType:(CMVideoCodecType)codecType {
    
    // 如果硬编码会话不存在，创建一个
    if (!compressionSession) {
        [self createCompressionSessionWithCodecType:codecType];
    }
    
    // 获取采样缓冲区的图像数据和时间戳
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    CMTime presentationTimeStamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
    
    // 开始编码采样缓冲区的图像数据
    OSStatus status = VTCompressionSessionEncodeFrame(compressionSession, imageBuffer, presentationTimeStamp, kCMTimeInvalid, NULL, NULL, NULL);
    if (status != noErr) {
        NSLog(@"Error: %@", @(status));
        return NULL;
    }
    
    // 等待编码完成
    VTCompressionSessionCompleteFrames(compressionSession, kCMTimeInvalid);
    
    // 获取编码后的图像数据
    CMSampleBufferRef encodedSampleBuffer = NULL;
    while (true) {
        // 从会话中获取编码后的图像数据
        CMSampleBufferRef sample = VTCompressionSessionCopyNextOutputSample(compressionSession);
        if (!sample) {
            break;
        }
        
        // 检查图像数据是否是关键帧
        CFDictionaryRef sampleAttachments = CMSampleBufferGetSampleAttachmentsArray(sample, true);
        CFDictionaryRef attachment = CFDictionaryGetValue(sampleAttachments, kCMSampleAttachmentKey_NotSync);
        bool isKeyFrame = !CFDictionaryContainsKey(attachment, kCFBooleanTrue);
        
        // 如果编码后的图像数据是关键帧，并且需要切换编码类型，则释放之前的会话，重新创建一个会话
        if (isKeyFrame && codecType != VTCompressionSessionGetEncoderType(compressionSession)) {
            [self destroyCompressionSession];
            [self createCompressionSessionWithCodecType:codecType];
        }
        
        // 返回编码后的图像数据
        encodedSampleBuffer = sample;
    }
    
    return encodedSampleBuffer;
}

// 创建硬编码会话
- (void)createCompressionSessionWithCodecType:(CMVideoCodecType)codecType {
    // 创建压缩会话属性
    CFMutableDictionaryRef sessionProperties = CFDictionaryCreateMutable(NULL, 0, &kCFTypeDictionaryKeyCallBacks, &kCFTypeDictionaryValueCallBacks);
    CFDictionarySetValue(sessionProperties, kVTCompressionPropertyKey_RealTime, kCFBooleanTrue);
    CFDictionarySetValue(sessionProperties, kVTCompressionPropertyKey_ProfileLevel, codecType == kCMVideoCodecType_H265 ? kVTProfileLevel_HEVC_Main_AutoLevel : kVTProfileLevel_H264_Main_AutoLevel);
    
    // 创建压缩会话
    OSStatus status = VTCompressionSessionCreate(NULL, 640, 480, codecType, sessionProperties, NULL, NULL, NULL, NULL, &compressionSession);
    if (status != noErr) {
        NSLog(@"Error: %@", @(status));
        return;
    }

````

---

### 四

以下是基于上述硬编过后的CMSampleBufferRef，保存在沙盒并进行播放，并验证使用的是哪种类型的硬编码的方法：



```objc
// 保存编码后的图像数据到沙盒中，并播放
- (void)saveEncodedSampleBuffer:(CMSampleBufferRef)sampleBuffer codecType:(CMVideoCodecType)codecType {
    // 创建文件路径
    NSString *documentsPath = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES).firstObject;
    NSString *filePath = [documentsPath stringByAppendingPathComponent:[NSString stringWithFormat:@"encoded.%@", codecType == kCMVideoCodecType_H265 ? @"hevc" : @"h264"]];
    
    // 创建文件管理器
    NSFileManager *fileManager = [NSFileManager defaultManager];
    
    // 删除旧文件
    if ([fileManager fileExistsAtPath:filePath]) {
        [fileManager removeItemAtPath:filePath error:nil];
    }
    
    // 打开文件
    BOOL success = [fileManager createFileAtPath:filePath contents:nil attributes:nil];
    if (!success) {
        NSLog(@"Failed to create file at path: %@", filePath);
        return;
    }
    
    // 获取图像数据并写入文件
    CMBlockBufferRef blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer);
    size_t length = CMBlockBufferGetDataLength(blockBuffer);
    NSMutableData *data = [NSMutableData dataWithLength:length];
    CMBlockBufferCopyDataBytes(blockBuffer, 0, length, data.mutableBytes);
    [data writeToFile:filePath atomically:YES];
    
    // 播放文件
    AVAsset *asset = [AVAsset assetWithURL:[NSURL fileURLWithPath:filePath]];
    AVPlayerItem *playerItem = [AVPlayerItem playerItemWithAsset:asset];
    AVPlayer *player = [AVPlayer playerWithPlayerItem:playerItem];
    AVPlayerLayer *playerLayer = [AVPlayerLayer playerLayerWithPlayer:player];
    playerLayer.frame = self.view.bounds;
    [self.view.layer addSublayer:playerLayer];
    [player play];
}

```





// 编码采样缓冲区
CMSampleBufferRef encodedSampleBuffer = [self encodeSampleBuffer:sampleBuffer codecType:kCMVideoCodecType_H264];

// 保存编码后的图像数据到沙盒中，并播放
[self saveEncodedSampleBuffer:encodedSampleBuffer codecType:VTCompressionSessionGetEncoderType(compressionSession)];





调用该方法后，会将编码后的图像数据保存到沙盒中，并使用AVPlayer进行播放。在播放过程中，我们可以观察视频的编码类型（如文件后缀名），以验证使用的是哪种类型的硬编码。

---







上述代码如何区分264还是265



ChatGPT



在iOS设备上，可以使用VideoToolbox框架解码H.264和H.265视频。当使用VideoToolbox进行硬解码时，需要为解码器指定视频格式，以便它可以正确地解码视频数据。

对于H.264视频，可以使用`kCMVideoCodecType_H264`作为视频编码格式：

```
objcCopy code
// 设置解码器参数
if (!_decompressionSession) {
    CMVideoFormatDescriptionRef formatDesc = NULL;
    CMVideoFormatDescriptionCreateFromH264ParameterSets(NULL, parameterSetCount, parameterSetPointers, parameterSetSizes, NALUnitHeaderLength, &formatDesc);
    VTDecompressionOutputCallbackRecord outputCallback = {VideoToolboxDecompressionCallback, (__bridge void *)self};
    VTDecompressionSessionCreate(NULL, formatDesc, NULL, NULL, &outputCallback, &_decompressionSession);
    CFRelease(formatDesc);
}
```

对于H.265视频，可以使用`kCMVideoCodecType_HEVC`作为视频编码格式：

```
objcCopy code
// 设置解码器参数
if (!_decompressionSession) {
    CMVideoFormatDescriptionRef formatDesc = NULL;
    CMVideoFormatDescriptionCreateFromHEVCParameterSets(NULL, parameterSetCount, parameterSetPointers, parameterSetSizes, NALUnitHeaderLength, NULL, &formatDesc);
    VTDecompressionOutputCallbackRecord outputCallback = {VideoToolboxDecompressionCallback, (__bridge void *)self};
    VTDecompressionSessionCreate(NULL, formatDesc, NULL, NULL, &outputCallback, &_decompressionSession);
    CFRelease(formatDesc);
}
```

需要注意的是，以上示例代码中的`parameterSetCount`，`parameterSetPointers`，`parameterSetSizes`和`NALUnitHeaderLength`都是针对特定编码格式的参数，需要根据具体情况进行设置。另外，对于HEVC编码的视频，还可以使用`CMVideoFormatDescriptionCreateFromHEVCCodecPrivate`函数来创建格式描述对象。